# Hybrid-RAG-Chatbot-for-Document-based-Q-A
 LangChain-based RAG chatbot to process documents- PDFs / TXT / MD / HTML files

ðŸ“„ðŸ”Ž Upload Documents â†’ Ask Questions (RAG + LangChain)

Objective: Build a local, no-LLM-API RAG app where users can upload PDFs/TXT/Markdown/HTML, the app indexes the content (split â†’ embed â†’ store), and then lets users ask questions whose answers are grounded in the uploaded documents. It emphasizes: Speed vs. accuracy presets (user can trade off). Hybrid retrieval (keyword BM25 + semantic FAISS) with optional cross-encoder reranking. Open-source generation (FLAN-T5 via Hugging Face), so it runs on CPU if needed.

A simple, friendly Streamlit UI.

***************************************************************************************************************************************************************************************************************************************************************************************** 
Frontend & Orchestration

Streamlit â€“ Provides an interactive web UI for uploading documents, setting parameters, and viewing results.
Streamlit Session State â€“ Stores documents, indexes, retrievers, and models across user interactions for efficiency.
Streamlit Cache Resource â€“ Prevents reloading heavy models on every request, improving responsiveness.
***********************************************************************************************************
Document Loading

PyPDFLoader â€“ Parses PDF files into structured text segments.
TextLoader â€“ Reads plain text (.txt) and markdown (.md) files into LangChain Document objects.
BSHTMLLoader â€“ Extracts readable text from HTML pages while preserving basic structure.
***********************************************************************************************************
Text Processing

MarkdownHeaderTextSplitter â€“ Splits content based on headings (#, ##, ###) to keep semantically related text together.
RecursiveCharacterTextSplitter â€“ Further divides text into size-controlled chunks with optional overlap for better context recall.
*********************************************************************************************************************************
Vectorization & Indexing

HuggingFaceEmbeddings â€“ Converts text chunks into dense vector embeddings for semantic similarity search.
FAISS (Facebook AI Similarity Search) â€“ High-performance vector database enabling fast approximate nearest neighbor search on embeddings.
*******************************************************************************************************************************************
Retrieval

BM25Retriever â€“ Keyword-based retrieval method that excels at matching exact terms, acronyms, and numbers.
FAISS Retriever with Maximal Marginal Relevance (MMR) â€“ Ensures retrieved chunks are both relevant and diverse.
EnsembleRetriever â€“ Combines BM25 and FAISS retrievers with weighted scoring to balance keyword and semantic matches.
***********************************************************************************************************************************
Reranking (Optional)

CrossEncoderReranker â€“ Uses a cross-encoder model to re-score and re-order top retrieved chunks for higher precision.
HuggingFaceCrossEncoder (ms-marco-MiniLM-L-6-v2) â€“ Pretrained cross-encoder specialized in passage relevance ranking.
***********************************************************************************************************************************
Language Model (LLM)

HuggingFace Transformers â€“ Framework for loading and running open-source sequence-to-sequence models.
FLAN-T5 (google/flan-t5-large / -xl) â€“ Instruction-tuned text generation model that can follow prompts and generate concise, grounded answers.
HuggingFacePipeline â€“ Wraps the Hugging Face model into a LangChain-compatible LLM interface.
***************************************************************************************************************************************************
Prompting & Chaining

ChatPromptTemplate â€“ Defines structured prompts with system instructions to keep answers grounded in provided context.
create_stuff_documents_chain â€“ Combines retrieved document chunks into a single context for the LLM.
create_retrieval_chain â€“ Links retriever and generation chain into a unified RAG pipeline.

